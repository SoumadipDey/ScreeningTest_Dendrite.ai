{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Qb_Fflqd1EJ3"
      ],
      "authorship_tag": "ABX9TyOZj2DVZzeWiaOZxI1l/NFP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoumadipDey/ScreeningTest_Dendrite.ai/blob/main/Dendrite_ai_Screening_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fetching the data and parameters**"
      ],
      "metadata": {
        "id": "Qb_Fflqd1EJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/SoumadipDey/ScreeningTest_Dendrite.ai/raw/a41fb019783d8820a1f5ca081013c4df2d98874f/algoparams_from_ui.json.rtf --quiet\n",
        "!wget https://github.com/SoumadipDey/ScreeningTest_Dendrite.ai/raw/a41fb019783d8820a1f5ca081013c4df2d98874f/iris.csv --quiet"
      ],
      "metadata": {
        "id": "5Yl4hbMN0V7e"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing packages**"
      ],
      "metadata": {
        "id": "HF-F0zAworpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install striprtf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77xJTS8houa0",
        "outputId": "a2ef1efd-217f-44f6-86e2-e5ae8cc5f5eb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: striprtf in /usr/local/lib/python3.10/dist-packages (0.0.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Priliminary Actions**"
      ],
      "metadata": {
        "id": "Q1O_g-BWN0Ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing the library functions**"
      ],
      "metadata": {
        "id": "oOAh1vXZN5QR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "XlIZA5pENosD"
      },
      "outputs": [],
      "source": [
        "import json as json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from striprtf.striprtf import rtf_to_text\n",
        "\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score,make_scorer\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor,ExtraTreesClassifier\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disabling Some Warnings**"
      ],
      "metadata": {
        "id": "FDq7v8c8Mq_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
      ],
      "metadata": {
        "id": "r3PZ3kOjMvdK"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Declaring a few globals**"
      ],
      "metadata": {
        "id": "mysQU9ijo4xQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IRIS_PATH = \"/content/iris.csv\"\n",
        "PARAMS_PATH = \"/content/algoparams_from_ui.json.rtf\""
      ],
      "metadata": {
        "id": "uk3XuGNHo8ry"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting `algoparams_from_ui.rtf` to Dictionary**"
      ],
      "metadata": {
        "id": "ukinlSNYoSCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convertRtfToDict(path: str) -> dict:\n",
        "  param_file = open(path,'r')\n",
        "  param_file_content = param_file.read()\n",
        "  param_file_content = rtf_to_text(param_file_content)\n",
        "  param_file.close()\n",
        "  return json.loads(param_file_content)\n",
        "allParams = convertRtfToDict(PARAMS_PATH)"
      ],
      "metadata": {
        "id": "me16FL9XoQH4"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting important parameters from the `allParams` Dictionary**"
      ],
      "metadata": {
        "id": "9iX9eP_Lz9_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_handling_params = allParams['design_state_data']['feature_handling']\n",
        "algorithm_params = allParams['design_state_data']['algorithms']\n",
        "feature_reduction_params = allParams['design_state_data']['feature_reduction']\n",
        "target_params = allParams['design_state_data']['target']"
      ],
      "metadata": {
        "id": "xPJuMLaAzfzR"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predType = target_params['prediction_type']\n",
        "targetFeature = target_params['target']\n",
        "featuresUsed = [feature for feature in feature_handling_params if feature_handling_params[feature]['is_selected']]\n",
        "algorithmsUsed = [algo for algo in algorithm_params if algorithm_params[algo]['is_selected']]"
      ],
      "metadata": {
        "id": "CncjD57ksfTG"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Features:\",featuresUsed)\n",
        "print(\"Target:\",targetFeature)\n",
        "print(\"Prediction type:\",predType)\n",
        "print(\"Algorithms used:\",algorithmsUsed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7I8YhCMxLHd",
        "outputId": "8c62eb92-6903-408d-b7e8-d4a880837f05"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
            "Target: petal_width\n",
            "Prediction type: Regression\n",
            "Algorithms used: ['RandomForestRegressor']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading and splitting the dataset as required**"
      ],
      "metadata": {
        "id": "vpgokvw13vPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadAndSplitDataset(path: str, target: str, features: list, val_split: float = 0.25, random_state : int = 42):\n",
        "  df = pd.read_csv(path)\n",
        "  y = df[[target]].values.reshape(-1,1)\n",
        "\n",
        "  if(target in features):\n",
        "    features.remove(target)\n",
        "\n",
        "  X_df = df.drop([target], axis = 1)[features]\n",
        "  featurePositions = {val:index for index,val in enumerate(X_df.columns)}\n",
        "\n",
        "  X = X_df.values\n",
        "  return train_test_split(X, y, test_size = val_split, random_state = random_state), featurePositions\n",
        "\n",
        "(X_train, X_test, y_train, y_test), featurePositions = loadAndSplitDataset(IRIS_PATH,targetFeature,featuresUsed)"
      ],
      "metadata": {
        "id": "evgwP2z_3yEC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Column Transformer for missing value imputation and Feature Hash encoding the Categorical feature**"
      ],
      "metadata": {
        "id": "7eS56vYj-gTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createImputersAndEncoders(features :list, target :str):\n",
        "  if(target in features):\n",
        "    features.remove(target)\n",
        "  transformerList = []\n",
        "  for feature in features:\n",
        "    feature_handling = feature_handling_params[feature]\n",
        "    if(feature_handling['feature_variable_type'] == \"numerical\"):\n",
        "      if(feature_handling['feature_details']['missing_values'] == \"Impute\"):\n",
        "        if(feature_handling['feature_details']['impute_with'] == \"Average of values\"):\n",
        "          imputer = SimpleImputer(strategy = 'mean')\n",
        "          transformerList.append((f'{feature}_imputer',imputer,[featurePositions[feature]]))\n",
        "        elif(feature_handling['feature_details']['impute_with'] == \"custom\"):\n",
        "          imputer = SimpleImputer(strategy = 'constant', fill_value = feature_handling['feature_details']['impute_value'])\n",
        "          transformerList.append((f'{feature}_imputer',imputer,[featurePositions[feature]]))\n",
        "    else:\n",
        "      if(feature_handling['feature_details']['text_handling'] == \"Tokenize and hash\"):\n",
        "        encoder = FeatureHasher(n_features = 2, input_type=\"string\")\n",
        "        transformerList.append((f'{feature}_encoder',encoder,[featurePositions[feature]]))\n",
        "\n",
        "  return transformerList"
      ],
      "metadata": {
        "id": "mvG2Yz_XK28E"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputeEncodeTransformer = ColumnTransformer(createImputersAndEncoders(featuresUsed,targetFeature), remainder='passthrough')"
      ],
      "metadata": {
        "id": "1wh5xnvz8IOq"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = imputeEncodeTransformer.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "d_1piacoZEjU"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building Custom Transformer classes for Feature Reduuction**"
      ],
      "metadata": {
        "id": "SCf05wmeUg9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NoReductionSelection(BaseEstimator, TransformerMixin):\n",
        "  '''\n",
        "  This transformer performs no feature reduction\n",
        "  '''\n",
        "  def fit(self, X, y=None):\n",
        "    self.X = X\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, *_):\n",
        "    return self.X"
      ],
      "metadata": {
        "id": "18vGMSlbUnDe"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CorrWithTargetSelection(BaseEstimator, TransformerMixin):\n",
        "  '''\n",
        "  This transformer performs feature selection based on colrrelation\n",
        "  of features with the target. The features with highest absolute\n",
        "  correlation with the target variable are selected.\n",
        "  '''\n",
        "  def __init__(self, n_features: int):\n",
        "    self.n_features = n_features\n",
        "    self.selected_columns = None\n",
        "\n",
        "  def fit(self, X, y = None):\n",
        "    combined_Xy = np.hstack((X,y)).astype(np.float32)\n",
        "    correlations = np.corrcoef(combined_Xy, rowvar=False)\n",
        "    self.selected_columns = np.argsort(np.abs(correlations[-1][:-1]))[- self.n_features:]\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None, **kwargs):\n",
        "    return X[ : , self.selected_columns]\n",
        "\n",
        "  def get_params(self, deep=False):\n",
        "    return {\"n_features\": self.n_features}\n"
      ],
      "metadata": {
        "id": "XjWg2TG2qplV"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TreeBasedSelection(BaseEstimator, TransformerMixin):\n",
        "  '''\n",
        "  This transformer performs feature selection based on feature importances generated\n",
        "  by a tree based estimator algorithm.\n",
        "  (`RandomForestRegressor` or `RandomForestClassifier` for this class.)\n",
        "  '''\n",
        "  def __init__(self, n_features: int, n_trees: int = 5, depth: int = 10, task_type: str = 'Regression'):\n",
        "    self.n_features = n_features\n",
        "    self.n_trees = n_trees\n",
        "    self.depth = depth\n",
        "    self.task_type = task_type\n",
        "    self.estm = None\n",
        "\n",
        "  def fit(self, X, y = None):\n",
        "    if self.task_type == 'Regression':\n",
        "      self.estm = RandomForestRegressor(n_estimators = self.n_trees,\n",
        "                                             max_depth = self.depth,\n",
        "                                             random_state = 42)\n",
        "    else:\n",
        "      self.estm = RandomForestClassifier(n_estimators = self.n_trees,\n",
        "                                              max_depth = self.depth,\n",
        "                                              random_state = 42)\n",
        "\n",
        "    self.estm.fit(X,y)\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y=None, **kwargs):\n",
        "    return SelectFromModel(self.estm, prefit=True,\n",
        "                           max_features=self.n_features,\n",
        "                           threshold = -np.inf).transform(X)\n",
        "\n",
        "  def get_params(self, deep=False):\n",
        "    return {\"n_features\": self.n_features, \"n_trees\": self.n_trees, \"depth\":self.depth}"
      ],
      "metadata": {
        "id": "QcUi2RklCBFZ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Column Transformer for Feature Reduction**"
      ],
      "metadata": {
        "id": "f9Xx4XRCv_Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def featureReducer():\n",
        "  params = feature_reduction_params\n",
        "  strategy = params['feature_reduction_method']\n",
        "  if(strategy == \"No Reduction\"):\n",
        "    return NoReductionSelection()\n",
        "  elif(strategy == \"Correlation with target\"):\n",
        "    n_features = int(params['num_of_features_to_keep'])\n",
        "    return CorrWithTargetSelection(n_features = n_features)\n",
        "  elif(strategy == \"Tree-based\"):\n",
        "    n_features = int(params['num_of_features_to_keep'])\n",
        "    n_trees = int(params['num_of_trees'])\n",
        "    depth = int(params['depth_of_trees'])\n",
        "    return TreeBasedSelection(n_features = n_features, n_trees = n_trees, depth = depth, task_type = predType)\n",
        "  elif(strategy == \"Principal Component Analysis\"):\n",
        "    n_features = int(params['num_of_features_to_keep'])\n",
        "    return PCA(n_components = n_features, random_state = 42)"
      ],
      "metadata": {
        "id": "1PMU31oiPRTT"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = featureReducer().fit_transform(X_new,y_train)"
      ],
      "metadata": {
        "id": "i9dhyb3hAdjs"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating an Estimator Type Dictionary which will make it easier for us to create appropriate estimators for a task**"
      ],
      "metadata": {
        "id": "dACkTA10nbha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimatorTypes = {  \"RandomForestClassifier\":\"Classification\",\n",
        "                    \"GBTClassifier\":\"Classification\",\n",
        "                    \"LogisticRegression\":\"Classification\",\n",
        "                    \"DecisionTreeClassifier\":\"Classification\",\n",
        "                    \"SVM\":\"Classification\",\n",
        "                    \"KNN\":\"Classification\",\n",
        "                    \"extra_random_trees\":\"Classification\",\n",
        "                    \"neural_network\":\"Classification\",\n",
        "                    \"RandomForestRegressor\":\"Regression\",\n",
        "                    \"GBTRegressor\":\"Regression\",\n",
        "                    \"LinearRegression\":\"Regression\",\n",
        "                    \"RidgeRegression\":\"Regression\",\n",
        "                    \"LassoRegression\":\"Regression\",\n",
        "                    \"ElasticNetRegression\":\"Regression\",\n",
        "                    \"DecisionTreeRegressor\":\"Regression\"\n",
        "                 }"
      ],
      "metadata": {
        "id": "9cUasw_GYFnG"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Estimator Building and GridSearching Function**"
      ],
      "metadata": {
        "id": "kFKdIUBTBbhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse_loss(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true,y_pred))\n",
        "\n",
        "rmse = make_scorer(rmse_loss, greater_is_better=False)"
      ],
      "metadata": {
        "id": "7TaDVtQEKMb-"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildBestEstimatorPipeline(estimatorName: str, X_train, y_train):\n",
        "  # ----------------------------------------------------------\n",
        "  # Common Parameters for Pipelines\n",
        "  # ----------------------------------------------------------\n",
        "  imputeEncodeTransformer = ColumnTransformer(createImputersAndEncoders(featuresUsed,targetFeature), remainder='passthrough')\n",
        "  featureReductionTransformer = featureReducer()\n",
        "  params = algorithm_params[estimatorName]\n",
        "  training_metrics_regression = {\"R2-Score\":None,\"RMSE\":None}\n",
        "  training_metrics_classification = {\"Accuracy\":None}\n",
        "  # ----------------------------------------------------------\n",
        "  # GridSearch and Pipeline Creation of various estimators\n",
        "  # ----------------------------------------------------------\n",
        "  if(estimatorName == \"DecisionTreeRegressor\"):\n",
        "    min_depth = int(params['min_depth'])\n",
        "    max_depth = int(params['max_depth'])\n",
        "    min_samples_leaf = params['min_samples_per_leaf']\n",
        "    split = []\n",
        "    if params['use_best']:\n",
        "            split.append('best')\n",
        "\n",
        "    if params['use_random']:\n",
        "            split.append('random')\n",
        "\n",
        "    pipe = Pipeline(steps = [ ('ImputerEncoder', imputeEncodeTransformer),\n",
        "                              ('FeatureReducer', featureReductionTransformer),\n",
        "                              ('Estimator', DecisionTreeRegressor())])\n",
        "    param_grid = {'Estimator__splitter':split,\n",
        "                  'Estimator__max_depth': range(min_depth, max_depth + 1),\n",
        "                  'Estimator__min_samples_leaf': min_samples_leaf}\n",
        "    grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid,\n",
        "                               scoring = rmse, cv = 2, n_jobs = -1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    y_pred_train = best_pipeline.predict(X_train)\n",
        "    training_metrics_regression[\"R2-Score\"] = r2_score(y_train,y_pred_train)\n",
        "    training_metrics_regression[\"RMSE\"] = rmse_loss(y_train,y_pred_train)\n",
        "    return best_pipeline,training_metrics_regression\n",
        "\n",
        "  elif(estimatorName == \"ElasticNetRegression\"):\n",
        "    min_iter = int(params['min_iter'])\n",
        "    max_iter = int(params['max_iter'])\n",
        "    min_regp = float(params['min_regparam'])\n",
        "    max_regp = float(params['max_regparam'])\n",
        "    min_enet = float(params['min_elasticnet'])\n",
        "    max_enet = float(params['max_elasticnet'])\n",
        "    pipe = Pipeline(steps = [ ('ImputerEncoder', imputeEncodeTransformer),\n",
        "                              ('FeatureReducer', featureReductionTransformer),\n",
        "                              ('Estimator', ElasticNet())])\n",
        "    param_grid = {'Estimator__max_iter': range(min_iter, max_iter + 1),\n",
        "                  'Estimator__l1_ratio': np.arange(min_enet, max_enet, 0.1),\n",
        "                  'Estimator__alpha': np.arange(min_regp, max_regp, 0.1)}\n",
        "    grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid,\n",
        "                               scoring = rmse, cv = 2, n_jobs = -1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    y_pred_train = best_pipeline.predict(X_train)\n",
        "    training_metrics_regression[\"R2-Score\"] = r2_score(y_train,y_pred_train)\n",
        "    training_metrics_regression[\"RMSE\"] = rmse_loss(y_train,y_pred_train)\n",
        "    return best_pipeline,training_metrics_regression\n",
        "\n",
        "\n",
        "  elif(estimatorName == \"LassoRegression\"):\n",
        "    min_iter = int(params['min_depth'])\n",
        "    max_iter = int(params['max_depth'])\n",
        "    min_regp = float(params['min_regparam'])\n",
        "    max_regp = float(params['max_regparam'])\n",
        "    pipe = Pipeline(steps = [ ('ImputerEncoder', imputeEncodeTransformer),\n",
        "                              ('FeatureReducer', featureReductionTransformer),\n",
        "                              ('Estimator', Lasso())])\n",
        "    param_grid = {'Estimator__max_iter': range(min_iter, max_iter + 1),\n",
        "                  'Estimator__alpha': np.arange(min_regp, max_regp, 0.1)}\n",
        "    grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid,\n",
        "                               scoring = rmse, cv = 2, n_jobs = -1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    y_pred_train = best_pipeline.predict(X_train)\n",
        "    training_metrics_regression[\"R2-Score\"] = r2_score(y_train,y_pred_train)\n",
        "    training_metrics_regression[\"RMSE\"] = rmse_loss(y_train,y_pred_train)\n",
        "    return best_pipeline,training_metrics_regression\n",
        "\n",
        "  elif(estimatorName == \"RidgeRegression\"):\n",
        "    min_iter = int(params['min_depth'])\n",
        "    max_iter = int(params['max_depth'])\n",
        "    min_regp = float(params['min_regparam'])\n",
        "    max_regp = float(params['max_regparam'])\n",
        "    pipe = Pipeline(steps = [ ('ImputerEncoder', imputeEncodeTransformer),\n",
        "                              ('FeatureReducer', featureReductionTransformer),\n",
        "                              ('Estimator', Ridge())])\n",
        "    param_grid = {'Estimator__max_iter': range(min_iter, max_iter + 1),\n",
        "                  'Estimator__alpha': np.arange(min_regp, max_regp, 0.1)}\n",
        "    grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid,\n",
        "                               scoring = rmse, cv = 2, n_jobs = -1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    y_pred_train = best_pipeline.predict(X_train)\n",
        "    training_metrics_regression[\"R2-Score\"] = r2_score(y_train,y_pred_train)\n",
        "    training_metrics_regression[\"RMSE\"] = rmse_loss(y_train,y_pred_train)\n",
        "    return best_pipeline,training_metrics_regression\n",
        "\n",
        "  elif(estimatorName == \"LinearRegression\"):\n",
        "    jobs = -1\n",
        "    pipe = Pipeline(steps = [ ('ImputerEncoder', imputeEncodeTransformer),\n",
        "                              ('FeatureReducer', featureReductionTransformer),\n",
        "                              ('Estimator', LinearRegression())])\n",
        "    param_grid = {'Estimator__n_jobs': [jobs]}\n",
        "    grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid,\n",
        "                               scoring = rmse, cv = 2, n_jobs = -1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    y_pred_train = best_pipeline.predict(X_train)\n",
        "    training_metrics_regression[\"R2-Score\"] = r2_score(y_train,y_pred_train)\n",
        "    training_metrics_regression[\"RMSE\"] = rmse_loss(y_train,y_pred_train)\n",
        "    return best_pipeline,training_metrics_regression\n",
        "\n",
        "  elif(estimatorName == \"GBTRegressor\"):\n",
        "    min_depth = int(params['min_depth'])\n",
        "    max_depth = int(params['max_depth'])\n",
        "    n_trees = params['num_of_BoostingStages']\n",
        "    if params['feature_sampling_statergy'] == \"Fixed number\":\n",
        "      max_features = [int(params['fixed_number'])]\n",
        "    pipe = Pipeline(steps = [ ('ImputerEncoder', imputeEncodeTransformer),\n",
        "                              ('FeatureReducer', featureReductionTransformer),\n",
        "                              ('Estimator', GradientBoostingRegressor())])\n",
        "    param_grid = {'Estimator__n_estimators': n_trees,\n",
        "                  'Estimator__max_depth': range(min_depth, max_depth + 1),\n",
        "                  'Estimator__max_features': max_features}\n",
        "    grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid,\n",
        "                               scoring = rmse, cv = 2, n_jobs = -1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    y_pred_train = best_pipeline.predict(X_train)\n",
        "    training_metrics_regression[\"R2-Score\"] = r2_score(y_train,y_pred_train)\n",
        "    training_metrics_regression[\"RMSE\"] = rmse_loss(y_train,y_pred_train)\n",
        "    return best_pipeline,training_metrics_regression\n",
        "\n",
        "  elif(estimatorName == \"RandomForestRegressor\"):\n",
        "    min_depth = int(params['min_depth'])\n",
        "    max_depth = int(params['max_depth'])\n",
        "    min_trees = int(params['min_trees'])\n",
        "    max_trees = int(params['max_trees'])\n",
        "    min_samples_leaf = range(int(params['min_samples_per_leaf_min_value']),int(params['min_samples_per_leaf_max_value']) + 1)\n",
        "    pipe = Pipeline(steps = [ ('ImputerEncoder', imputeEncodeTransformer),\n",
        "                              ('FeatureReducer', featureReductionTransformer),\n",
        "                              ('Estimator', RandomForestRegressor())])\n",
        "    param_grid = {'Estimator__n_estimators': range(min_trees, max_trees + 1),\n",
        "                  'Estimator__max_depth': range(min_depth, max_depth + 1),\n",
        "                  'Estimator__min_samples_leaf': min_samples_leaf,\n",
        "                  'Estimator__n_jobs': [-1]}\n",
        "    grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid,\n",
        "                               scoring = rmse, cv = 2, n_jobs = -1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    y_pred_train = best_pipeline.predict(X_train)\n",
        "    training_metrics_regression[\"R2-Score\"] = r2_score(y_train,y_pred_train)\n",
        "    training_metrics_regression[\"RMSE\"] = rmse_loss(y_train,y_pred_train)\n",
        "    return best_pipeline,training_metrics_regression\n",
        "\n",
        "  elif(estimatorName == \"neural_network\"):\n",
        "    hidden_layer_sizes = params['hidden_layer_sizes']\n",
        "    alpha = [float(params['alpha_value'])]\n",
        "    beta_1 = [float(params['beta_1'])]\n",
        "    beta_2 = [float(params['beta_2'])]\n",
        "    momentum = [float(params['momentum'])]\n",
        "    max_iter = [int(params['max_iterations'])]\n",
        "    shuffle = [params['shuffle_data']]\n",
        "    tol = [float(params['convergence_tolerance'])]\n",
        "    lr_init = [float(params['initial_learning_rate'])]\n",
        "    power_t = [float(params['power_t'])]\n",
        "    early_stopping = [params['early_stopping']]\n",
        "    nesterovs_momentum = [params['use_nesterov_momentum']]\n",
        "\n",
        "    if(str(params['activation']).lower() in ['identity','logistic','tanh','relu']):\n",
        "      activation = [str(params['activation']).lower()]\n",
        "    else:\n",
        "      activation = ['relu']\n",
        "\n",
        "    if(str(params['solver']).lower() in ['lbfgs','sgd','adam']):\n",
        "      solver = [str(params['solver']).lower()]\n",
        "    else:\n",
        "      solver = ['adam']\n",
        "\n",
        "    pipe = Pipeline(steps = [ ('ImputerEncoder', imputeEncodeTransformer),\n",
        "                              ('FeatureReducer', featureReductionTransformer),\n",
        "                              ('Estimator', MLPClassifier())])\n",
        "    param_grid = {'Estimator__hidden_layer_sizes': hidden_layer_sizes,\n",
        "                  'Estimator__alpha': alpha,\n",
        "                  'Estimator__beta_1': beta_1,\n",
        "                  'Estimator__beta_2': beta_2,\n",
        "                  'Estimator__momentum': momentum,\n",
        "                  'Estimator__max_iter': max_iter,\n",
        "                  'Estimator__shuffle': shuffle,\n",
        "                  'Estimator__tol': tol,\n",
        "                  'Estimator__learning_rate_init': lr_init,\n",
        "                  'Estimator__power_t': power_t,\n",
        "                  'Estimator__early_stopping': early_stopping,\n",
        "                  'Estimator__nesterovs_momentum': nesterovs_momentum,\n",
        "                  'Estimator__activation': activation,\n",
        "                  'Estimator__solver': solver}\n",
        "    grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid,\n",
        "                               scoring = 'accuracy', cv = 2, n_jobs = -1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    y_pred_train = best_pipeline.predict(X_train)\n",
        "    training_metrics_classification[\"Accuracy\"] = accuracy_score(y_train,y_pred_train)\n",
        "    return best_pipeline,training_metrics_classification\n",
        "\n",
        "  elif(estimatorName == \"extra_random_trees\"):\n",
        "    n_estimators = params['num_of_trees']\n",
        "    max_depth = params['max_depth']\n",
        "    min_samples_leaf = params['min_samples_per_leaf']\n",
        "    max_features = [None]\n",
        "\n",
        "    if(\"square\" in params['feature_sampling_strategy'].lower()):\n",
        "      max_features.append(\"sqrt\")\n",
        "      if(None in max_features):\n",
        "        max_features.remove(None)\n",
        "\n",
        "    if(\"log\" in params['feature_sampling_strategy'].lower()):\n",
        "      max_features.append(\"log2\")\n",
        "      if(None in max_features):\n",
        "        max_features.remove(None)\n",
        "\n",
        "    pipe = Pipeline(steps = [ ('ImputerEncoder', imputeEncodeTransformer),\n",
        "                              ('FeatureReducer', featureReductionTransformer),\n",
        "                              ('Estimator', ExtraTreesClassifier())])\n",
        "    param_grid = {'Estimator__n_estimators': n_estimators,\n",
        "                  'Estimator__max_depth': max_depth,\n",
        "                  'Estimator__min_samples_leaf': min_samples_leaf,\n",
        "                  'Estimator__max_features': max_features,\n",
        "                  'Estimator__n_jobs': [-1]}\n",
        "    grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid,\n",
        "                               scoring = 'accuracy', cv = 2, n_jobs = -1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    y_pred_train = best_pipeline.predict(X_train)\n",
        "    training_metrics_classification[\"Accuracy\"] = accuracy_score(y_train,y_pred_train)\n",
        "    return best_pipeline,training_metrics_classification\n",
        "\n",
        "  elif(estimatorName == \"KNN\"):\n",
        "    n_neighbors = params['k_value']\n",
        "    if(params['distance_weighting']):\n",
        "      weights = ['distance']\n",
        "    else:\n",
        "      weights = ['uniform']\n",
        "    p_value = [int(params['p_value'])]\n",
        "    pipe = Pipeline(steps = [ ('ImputerEncoder', imputeEncodeTransformer),\n",
        "                              ('FeatureReducer', featureReductionTransformer),\n",
        "                              ('Estimator', KNeighborsClassifier())])\n",
        "    param_grid = {'Estimator__n_neighbors': n_neighbors,\n",
        "                  'Estimator__weights': weights,\n",
        "                  'Estimator__p': p_value}\n",
        "    grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid,\n",
        "                               scoring = 'accuracy', cv = 2, n_jobs = -1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    y_pred_train = best_pipeline.predict(X_train)\n",
        "    training_metrics_classification[\"Accuracy\"] = accuracy_score(y_train,y_pred_train)\n",
        "    return best_pipeline,training_metrics_classification\n",
        "\n",
        "  elif(estimatorName == \"SVM\"):\n",
        "    c_value = params['c_value']\n",
        "    tol = [float(params['tolerance'])]\n",
        "    max_iter = [int(params['max_iterations'])]\n",
        "\n",
        "    kernel = []\n",
        "    if(params['linear_kernel']):\n",
        "      kernel.append('linear')\n",
        "    if(params['polynomial_kernel']):\n",
        "      kernel.append('poly')\n",
        "    if(params['rep_kernel']):\n",
        "      kernel.append('rbf')\n",
        "    if(params['sigmoid_kernel']):\n",
        "      kernel.append('sigmoid')\n",
        "    if(len(kernel) == 0):\n",
        "      kernel.append('rbf')\n",
        "\n",
        "    gamma = []\n",
        "    if(params['auto']):\n",
        "      gamma.append('auto')\n",
        "    if(params['scale']):\n",
        "      gamma.append('scale')\n",
        "    if(len(gamma) == 0):\n",
        "      gamma.append('scale')\n",
        "\n",
        "    pipe = Pipeline(steps = [ ('ImputerEncoder', imputeEncodeTransformer),\n",
        "                              ('FeatureReducer', featureReductionTransformer),\n",
        "                              ('Estimator', SVC())])\n",
        "    param_grid = {'Estimator__C': c_value,\n",
        "                  'Estimator__tol': tol,\n",
        "                  'Estimator__max_iter': max_iter,\n",
        "                  'Estimator__kernel': kernel,\n",
        "                  'Estimator__gamma': gamma}\n",
        "    grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid,\n",
        "                               scoring = 'accuracy', cv = 2, n_jobs = -1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    y_pred_train = best_pipeline.predict(X_train)\n",
        "    training_metrics_classification[\"Accuracy\"] = accuracy_score(y_train,y_pred_train)\n",
        "    return best_pipeline,training_metrics_classification\n",
        "\n",
        "  elif(estimatorName == \"DecisionTreeClassifier\"):\n",
        "\n",
        "    if (not(params['use_gini']) and params['use_entropy']):\n",
        "      criterion = 'entropy'\n",
        "    elif(params['use_gini'] and not(params['use_entropy'])):\n",
        "      criterion = 'gini'\n",
        "\n",
        "    min_depth = int(params['min_depth'])\n",
        "    max_depth = int(params['max_depth'])\n",
        "    min_samples_leaf = params['min_samples_per_leaf']\n",
        "    split = []\n",
        "    if params['use_best']:\n",
        "            split.append('best')\n",
        "\n",
        "    if params['use_random']:\n",
        "            split.append('random')\n",
        "\n",
        "    pipe = Pipeline(steps = [ ('ImputerEncoder', imputeEncodeTransformer),\n",
        "                              ('FeatureReducer', featureReductionTransformer),\n",
        "                              ('Estimator', DecisionTreeClassifier())])\n",
        "    param_grid = {'Estimator__splitter':split,\n",
        "                  'Estimator__criterion': [criterion],\n",
        "                  'Estimator__max_depth': range(min_depth, max_depth + 1),\n",
        "                  'Estimator__min_samples_leaf': min_samples_leaf}\n",
        "    grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid,\n",
        "                               scoring = 'accuracy', cv = 2, n_jobs = -1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    y_pred_train = best_pipeline.predict(X_train)\n",
        "    training_metrics_classification[\"Accuracy\"] = accuracy_score(y_train,y_pred_train)\n",
        "    return best_pipeline,training_metrics_classification\n",
        "\n",
        "  elif(estimatorName == \"LogisticRegression\"):\n",
        "    min_iter = int(params['min_depth'])\n",
        "    max_iter = int(params['max_depth'])\n",
        "    pipe = Pipeline(steps = [ ('ImputerEncoder', imputeEncodeTransformer),\n",
        "                              ('FeatureReducer', featureReductionTransformer),\n",
        "                              ('Estimator', LogisticRegression())])\n",
        "    param_grid = {'Estimator__max_iter': range(min_iter, max_iter + 1),\n",
        "                  'Estimator__n_jobs': [-1]}\n",
        "    grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid,\n",
        "                               scoring = 'accuracy', cv = 2, n_jobs = -1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    y_pred_train = best_pipeline.predict(X_train)\n",
        "    training_metrics_classification[\"Accuracy\"] = accuracy_score(y_train,y_pred_train)\n",
        "    return best_pipeline,training_metrics_classification\n",
        "\n",
        "  elif(estimatorName == \"GBTClassifier\"):\n",
        "    min_depth = int(params['min_depth'])\n",
        "    max_depth = int(params['max_depth'])\n",
        "    n_trees = params['num_of_BoostingStages']\n",
        "    if params['feature_sampling_statergy'] == \"Fixed number\":\n",
        "      max_features = [int(params['fixed_number'])]\n",
        "    pipe = Pipeline(steps = [ ('ImputerEncoder', imputeEncodeTransformer),\n",
        "                              ('FeatureReducer', featureReductionTransformer),\n",
        "                              ('Estimator', GradientBoostingClassifier())])\n",
        "    param_grid = {'Estimator__n_estimators': n_trees,\n",
        "                  'Estimator__max_depth': range(min_depth, max_depth + 1),\n",
        "                  'Estimator__max_features': max_features}\n",
        "    grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid,\n",
        "                               scoring = 'accuracy', cv = 2, n_jobs = -1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    y_pred_train = best_pipeline.predict(X_train)\n",
        "    training_metrics_classification[\"Accuracy\"] = accuracy_score(y_train,y_pred_train)\n",
        "    return best_pipeline,training_metrics_classification\n",
        "\n",
        "  elif(estimatorName == \"RandomForestClassifier\"):\n",
        "    min_depth = int(params['min_depth'])\n",
        "    max_depth = int(params['max_depth'])\n",
        "    min_trees = int(params['min_trees'])\n",
        "    max_trees = int(params['max_trees'])\n",
        "    min_samples_leaf = range(int(params['min_samples_per_leaf_min_value']),int(params['min_samples_per_leaf_max_value']) + 1)\n",
        "    pipe = Pipeline(steps = [ ('ImputerEncoder', imputeEncodeTransformer),\n",
        "                              ('FeatureReducer', featureReductionTransformer),\n",
        "                              ('Estimator', RandomForestClassifier())])\n",
        "    param_grid = {'Estimator__n_estimators': range(min_trees, max_trees + 1),\n",
        "                  'Estimator__max_depth': range(min_depth, max_depth + 1),\n",
        "                  'Estimator__min_samples_leaf': min_samples_leaf,\n",
        "                  'Estimator__n_jobs': [-1]}\n",
        "    grid_search = GridSearchCV(estimator = pipe, param_grid = param_grid,\n",
        "                               scoring = 'accuracy', cv = 2, n_jobs = -1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_pipeline = grid_search.best_estimator_\n",
        "    y_pred_train = best_pipeline.predict(X_train)\n",
        "    training_metrics_classification[\"Accuracy\"] = accuracy_score(y_train,y_pred_train)\n",
        "    return best_pipeline,training_metrics_classification\n",
        "\n"
      ],
      "metadata": {
        "id": "qdCsZauBBiUq"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildAndTestModels(algorithms, X_train,X_test,y_train,y_test) -> list:\n",
        "  trainedModels = []\n",
        "  for index,algorithm in enumerate(algorithms):\n",
        "    if(algorithm in estimatorTypes.keys()):\n",
        "      if(estimatorTypes[algorithm] == predType):\n",
        "        model,training_perf = buildBestEstimatorPipeline(algorithm,X_train,y_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "        if(predType == \"Regression\"):\n",
        "          test_perf = {\"R2-Score\":r2_score(y_test, y_pred_test),\"RMSE\":rmse_loss(y_test, y_pred_test)}\n",
        "        else:\n",
        "          test_perf = {\"Accuracy\":accuracy_score(y_test,y_pred_test)}\n",
        "        print(f'[{index + 1}] Algorithm: {algorithm}\\nTraining performance :\\n{training_perf}\\n\\nTest performance :\\n{test_perf}\\n',\n",
        "            end = '-----' * 5 + '\\n')\n",
        "      else:\n",
        "        print(f'\\n[{index + 1}] {algorithm} is invalid model type for {predType}\\n', end = '-----' * 5 + '\\n')\n",
        "    else:\n",
        "      print(f'\\n[{index + 1}] {algorithm} is currently undefined!\\n', end = '-----' * 5 + '\\n')"
      ],
      "metadata": {
        "id": "fEKEEr5OoWVC"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buildAndTestModels(algorithmsUsed, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "byuOpF26FtTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buildAndTestModels(['RandomForestRegressor', 'ElasticNetRegression', 'xg_boost', 'DecisionTreeRegressor', 'SVM', 'GBTRegressor'],\n",
        "                   X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11eerVTDB5-3",
        "outputId": "ea4acec0-00dc-47ba-f08a-852090d84740"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Algorithm: RandomForestRegressor\n",
            "Training performance :\n",
            "{'R2-Score': 0.9651068063500635, 'RMSE': 0.1393284845781922}\n",
            "\n",
            "Test performance :\n",
            "{'R2-Score': 0.9577309717232548, 'RMSE': 0.16465778471309317}\n",
            "-------------------------\n",
            "[2] Algorithm: ElasticNetRegression\n",
            "Training performance :\n",
            "{'R2-Score': 0.857394688602187, 'RMSE': 0.2816679310784978}\n",
            "\n",
            "Test performance :\n",
            "{'R2-Score': 0.8692636925023356, 'RMSE': 0.28958045323726456}\n",
            "-------------------------\n",
            "\n",
            "[3] xg_boost is currently undefined!\n",
            "-------------------------\n",
            "[4] Algorithm: DecisionTreeRegressor\n",
            "Training performance :\n",
            "{'R2-Score': 0.96569429412462, 'RMSE': 0.13815058651462378}\n",
            "\n",
            "Test performance :\n",
            "{'R2-Score': 0.936059022587885, 'RMSE': 0.2025166671586817}\n",
            "-------------------------\n",
            "\n",
            "[5] SVM is invalid model type for Regression\n",
            "-------------------------\n",
            "[6] Algorithm: GBTRegressor\n",
            "Training performance :\n",
            "{'R2-Score': 0.9986354996697301, 'RMSE': 0.027552205924898095}\n",
            "\n",
            "Test performance :\n",
            "{'R2-Score': 0.9301279813472784, 'RMSE': 0.21170093759375025}\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating the pipeline**"
      ],
      "metadata": {
        "id": "8V1V6bjZ8dtc"
      }
    }
  ]
}